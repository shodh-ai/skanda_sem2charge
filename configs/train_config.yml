# Training hyperparameters
training:
  epochs: 10
  batch_size: 16
  num_workers: 4
  accelerator: "gpu"
  devices: -1 # Use all available GPUs
  precision: 16
  gradient_clip_val: 1.0

  # Early stopping
  early_stopping:
    monitor: "val_loss"
    patience: 15
    mode: "min"

  # Model checkpointing
  checkpoint:
    monitor: "val_loss"
    save_top_k: 3
    mode: "min"

# Optimizer
optimizer:
  name: "adamw"
  lr: 0.001
  weight_decay: 0.0001

# Scheduler
scheduler:
  name: "reduce_on_plateau"
  patience: 10
  factor: 0.5
  min_lr: 1.0e-6

# Loss weights
loss_weights:
  microstructure: 1.0
  performance: 1.0
