# Training hyperparameters
training:
  epochs: 100
  batch_size: 32
  num_workers: 24
  accelerator: "gpu"
  devices: -1
  precision: "16-mixed"
  gradient_clip_val: 1.0

  # Early stopping
  early_stopping:
    monitor: "val_loss"
    patience: 20
    mode: "min"
    save_last: true

  # Model checkpointing
  checkpoint:
    monitor: "val_loss"
    save_top_k: 3
    mode: "min"

# Optimizer
optimizer:
  name: "adamw"
  lr: 0.0003
  weight_decay: 0.01
  betas: [0.9, 0.999]

# Scheduler
scheduler:
  name: "reduce_on_plateau"
  patience: 15
  factor: 0.5
  min_lr: 1.0e-7
