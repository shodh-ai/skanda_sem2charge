{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9cfb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RAW DATA VALIDATION & ANALYSIS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RAW DATA VALIDATION & ANALYSIS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91545623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Total samples: 100\n",
      "  Params per sample: 60\n",
      "  Expected total rows: 6000\n",
      "  Input features: 10\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\"..\")\n",
    "config_dir = BASE_DIR / \"configs\"\n",
    "\n",
    "with open(config_dir / \"optimise_config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "with open(config_dir / \"paths.yml\", \"r\") as f:\n",
    "    paths = yaml.safe_load(f)\n",
    "\n",
    "data_base = BASE_DIR / paths[\"data\"][\"base_dir\"]\n",
    "tau_csv = data_base / paths[\"data\"][\"input\"][\"tau_results_csv\"]\n",
    "parquet_dir = data_base / paths[\"data\"][\"input\"][\"parquet_dir\"]\n",
    "parquet_pattern = paths[\"data\"][\"input\"][\"parquet_pattern\"]\n",
    "images_dir = data_base / paths[\"data\"][\"input\"][\"images_dir\"]\n",
    "image_pattern = paths[\"data\"][\"input\"][\"image_pattern\"]\n",
    "\n",
    "TOTAL_SAMPLES = config[\"data\"][\"total_samples\"]\n",
    "PARAMS_PER_SAMPLE = config[\"data\"][\"params_per_sample\"]\n",
    "INPUT_FEATURES = config[\"data\"][\"input_features\"]\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Total samples: {TOTAL_SAMPLES}\")\n",
    "print(f\"  Params per sample: {PARAMS_PER_SAMPLE}\")\n",
    "print(f\"  Expected total rows: {TOTAL_SAMPLES * PARAMS_PER_SAMPLE}\")\n",
    "print(f\"  Input features: {len(INPUT_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccadcab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tau_results.csv...\n",
      "\n",
      "Tau results loaded:\n",
      "  Rows: 100\n",
      "  Columns: ['id', 'filename', 'porosity_measured', 'tau_factor', 'D_eff', 'error']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>porosity_measured</th>\n",
       "      <th>tau_factor</th>\n",
       "      <th>D_eff</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sample_0000.tif</td>\n",
       "      <td>0.519418</td>\n",
       "      <td>1.558766</td>\n",
       "      <td>0.333224</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sample_0001.tif</td>\n",
       "      <td>0.576339</td>\n",
       "      <td>2.387288</td>\n",
       "      <td>0.241420</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sample_0002.tif</td>\n",
       "      <td>0.525229</td>\n",
       "      <td>2.272830</td>\n",
       "      <td>0.231090</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sample_0003.tif</td>\n",
       "      <td>0.415018</td>\n",
       "      <td>2.483961</td>\n",
       "      <td>0.167079</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sample_0004.tif</td>\n",
       "      <td>0.438148</td>\n",
       "      <td>3.700079</td>\n",
       "      <td>0.118416</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id         filename  porosity_measured  tau_factor     D_eff  error\n",
       "0   0  sample_0000.tif           0.519418    1.558766  0.333224    NaN\n",
       "1   1  sample_0001.tif           0.576339    2.387288  0.241420    NaN\n",
       "2   2  sample_0002.tif           0.525229    2.272830  0.231090    NaN\n",
       "3   3  sample_0003.tif           0.415018    2.483961  0.167079    NaN\n",
       "4   4  sample_0004.tif           0.438148    3.700079  0.118416    NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading tau_results.csv...\")\n",
    "tau_df = pd.read_csv(tau_csv)\n",
    "\n",
    "print(f\"\\nTau results loaded:\")\n",
    "print(f\"  Rows: {len(tau_df)}\")\n",
    "print(f\"  Columns: {list(tau_df.columns)}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "tau_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ee86e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TAU RESULTS STATISTICS\n",
      "================================================================================\n",
      "            D_eff  porosity_measured  tau_factor\n",
      "count  100.000000         100.000000  100.000000\n",
      "mean     0.210705           0.487339   27.388088\n",
      "std      0.184230           0.180468  114.703536\n",
      "min      0.000154           0.152606    1.179202\n",
      "25%      0.041231           0.333235    1.801661\n",
      "50%      0.164922           0.494411    3.085141\n",
      "75%      0.348979           0.651169    7.657228\n",
      "max      0.633595           0.815443  989.615723\n",
      "\n",
      "Missing values in tau_results:\n",
      "id                     0\n",
      "filename               0\n",
      "porosity_measured      0\n",
      "tau_factor             0\n",
      "D_eff                  0\n",
      "error                100\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TAU RESULTS STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tau_stats = tau_df[[\"D_eff\", \"porosity_measured\", \"tau_factor\"]].describe()\n",
    "print(tau_stats)\n",
    "\n",
    "print(\"\\nMissing values in tau_results:\")\n",
    "print(tau_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5d9be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DISCOVERING PARQUET FILES\n",
      "================================================================================\n",
      "Found 100/100 parquet files\n",
      "Expected total rows: 6000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DISCOVERING PARQUET FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "parquet_files = []\n",
    "for sample_id in range(TOTAL_SAMPLES):\n",
    "    pq_file = parquet_dir / parquet_pattern.format(sample_id)\n",
    "    if pq_file.exists():\n",
    "        parquet_files.append(sample_id)\n",
    "\n",
    "print(f\"Found {len(parquet_files)}/{TOTAL_SAMPLES} parquet files\")\n",
    "print(f\"Expected total rows: {len(parquet_files) * PARAMS_PER_SAMPLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "197384f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING ALL PARQUET FILES (This may take 30-60 seconds...)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading parquet files: 100%|██████████| 100/100 [00:00<00:00, 181.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All parquet files loaded successfully\n",
      "\n",
      "Combined raw data:\n",
      "  Total rows: 6,000\n",
      "  Expected: 6,000\n",
      "  Unique samples: 100\n",
      "  Unique params: 60\n",
      "  Columns: 27\n",
      "  Memory usage: 6.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING ALL PARQUET FILES (This may take 30-60 seconds...)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_data = []\n",
    "load_errors = []\n",
    "\n",
    "for sample_id in tqdm(parquet_files, desc=\"Loading parquet files\"):\n",
    "    try:\n",
    "        pq_file = parquet_dir / parquet_pattern.format(sample_id)\n",
    "        df = pd.read_parquet(pq_file)\n",
    "        df[\"sample_id\"] = sample_id\n",
    "        all_data.append(df)\n",
    "    except Exception as e:\n",
    "        load_errors.append({\"sample_id\": sample_id, \"error\": str(e)})\n",
    "\n",
    "if load_errors:\n",
    "    print(f\"\\nFailed to load {len(load_errors)} parquet files\")\n",
    "    print(pd.DataFrame(load_errors))\n",
    "else:\n",
    "    print(\"\\nAll parquet files loaded successfully\")\n",
    "\n",
    "df_raw = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "print(f\"\\nCombined raw data:\")\n",
    "print(f\"  Total rows: {len(df_raw):,}\")\n",
    "print(f\"  Expected: {len(parquet_files) * PARAMS_PER_SAMPLE:,}\")\n",
    "print(f'  Unique samples: {df_raw[\"sample_id\"].nunique()}')\n",
    "print(f'  Unique params: {df_raw[\"param_id\"].nunique()}')\n",
    "print(f\"  Columns: {len(df_raw.columns)}\")\n",
    "print(f\"  Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b335c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RAW DATA SCHEMA\n",
      "================================================================================\n",
      " 1. sample_id                                (int64)\n",
      " 2. param_id                                 (int64)\n",
      " 3. filename                                 (object)\n",
      " 4. bruggeman_derived                        (float64)\n",
      " 5. input_param_id                           (int64)\n",
      " 6. input_SEI kinetic rate constant [m.s-1]  (float64)\n",
      " 7. input_Electrolyte diffusivity [m2.s-1]   (float64)\n",
      " 8. input_Initial concentration in electrolyte [mol.m-3] (int64)\n",
      " 9. input_Separator porosity                 (float64)\n",
      "10. input_Separator Bruggeman coefficient (electrolyte) (float64)\n",
      "11. input_Separator Bruggeman coefficient    (float64)\n",
      "12. input_Positive particle radius [m]       (float64)\n",
      "13. input_Negative particle radius [m]       (float64)\n",
      "14. input_Positive electrode thickness [m]   (float64)\n",
      "15. input_Negative electrode thickness [m]   (float64)\n",
      "16. status                                   (object)\n",
      "17. runtime_s                                (float64)\n",
      "18. error_message                            (object)\n",
      "19. capacity_trend_cycles                    (object)\n",
      "20. capacity_trend_ah                        (object)\n",
      "21. nominal_capacity_Ah                      (float64)\n",
      "22. eol_cycle_measured                       (float64)\n",
      "23. eol_cycle_predicted                      (float64)\n",
      "24. final_RUL                                (float64)\n",
      "25. cycle_first                              (object)\n",
      "26. cycle_middle                             (object)\n",
      "27. cycle_last                               (object)\n",
      "\n",
      "EXPECTED FIELDS VALIDATION\n",
      "✓ input_SEI kinetic rate constant [m.s-1]\n",
      "✓ input_Electrolyte diffusivity [m2.s-1]\n",
      "✓ input_Initial concentration in electrolyte [mol.m-3]\n",
      "✓ input_Separator porosity\n",
      "✓ input_Separator Bruggeman coefficient (electrolyte)\n",
      "✓ input_Separator Bruggeman coefficient\n",
      "✓ input_Positive particle radius [m]\n",
      "✓ input_Negative particle radius [m]\n",
      "✓ input_Positive electrode thickness [m]\n",
      "✓ input_Negative electrode thickness [m]\n",
      "✓ param_id\n",
      "✓ bruggeman_derived\n",
      "✓ nominal_capacity_Ah\n",
      "✓ eol_cycle_measured\n",
      "✓ capacity_trend_ah\n",
      "✓ final_RUL\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RAW DATA SCHEMA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, col in enumerate(df_raw.columns, 1):\n",
    "    print(f\"{i:2d}. {col:40s} ({df_raw[col].dtype})\")\n",
    "\n",
    "EXPECTED_FIELDS = INPUT_FEATURES + [\n",
    "    \"param_id\",\n",
    "    \"bruggeman_derived\",\n",
    "    \"nominal_capacity_Ah\",\n",
    "    \"eol_cycle_measured\",\n",
    "    \"capacity_trend_ah\",\n",
    "    \"final_RUL\",\n",
    "]\n",
    "\n",
    "print(\"\\nEXPECTED FIELDS VALIDATION\")\n",
    "for field in EXPECTED_FIELDS:\n",
    "    print((\"✓\" if field in df_raw.columns else \"✗\"), field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2bec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      count          mean           std           min           25%           50%           75%           max  missing  missing_pct\n",
      "feature                                                                                                                                                                            \n",
      "input_SEI kinetic rate constant [m.s-1]                6000  1.449557e-13  2.271902e-13  1.080000e-15  5.907500e-15  3.090000e-14  1.735000e-13  9.230000e-13        0          0.0\n",
      "input_Electrolyte diffusivity [m2.s-1]                 6000  3.001000e-10  5.789803e-11  2.000000e-10  2.515000e-10  3.010000e-10  3.497500e-10  3.980000e-10        0          0.0\n",
      "input_Initial concentration in electrolyte [mol.m-3]   6000  1.000000e+03  0.000000e+00  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03  1.000000e+03        0          0.0\n",
      "input_Separator porosity                               6000  5.870974e-01  2.892336e-02  5.380924e-01  5.628673e-01  5.867180e-01  6.123321e-01  6.363797e-01        0          0.0\n",
      "input_Separator Bruggeman coefficient (electrolyte)    6000  2.667379e+00  1.542638e-01  2.404702e+00  2.538698e+00  2.669024e+00  2.801685e+00  2.928699e+00        0          0.0\n",
      "input_Separator Bruggeman coefficient                  6000  2.667379e+00  1.542638e-01  2.404702e+00  2.538698e+00  2.669024e+00  2.801685e+00  2.928699e+00        0          0.0\n",
      "input_Positive particle radius [m]                     6000  5.221000e-06  6.043287e-07  4.200000e-06  4.707500e-06  5.215000e-06  5.747500e-06  6.260000e-06        0          0.0\n",
      "input_Negative particle radius [m]                     6000  5.859167e-06  6.772573e-07  4.710000e-06  5.290000e-06  5.870000e-06  6.422500e-06  7.010000e-06        0          0.0\n",
      "input_Positive electrode thickness [m]                 6000  7.559667e-05  4.381802e-06  6.820000e-05  7.185000e-05  7.560000e-05  7.942500e-05  8.310000e-05        0          0.0\n",
      "input_Negative electrode thickness [m]                 6000  8.520667e-05  4.928895e-06  7.690000e-05  8.107500e-05  8.515000e-05  8.947500e-05  9.350000e-05        0          0.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_comprehensive_stats(df, features):\n",
    "    stats = []\n",
    "    for feat in features:\n",
    "        if feat in df.columns:\n",
    "            col = df[feat]\n",
    "            stats.append(\n",
    "                {\n",
    "                    \"feature\": feat,\n",
    "                    \"count\": col.count(),\n",
    "                    \"mean\": (\n",
    "                        col.mean() if pd.api.types.is_numeric_dtype(col) else np.nan\n",
    "                    ),\n",
    "                    \"std\": col.std() if pd.api.types.is_numeric_dtype(col) else np.nan,\n",
    "                    \"min\": col.min() if pd.api.types.is_numeric_dtype(col) else np.nan,\n",
    "                    \"25%\": (\n",
    "                        col.quantile(0.25)\n",
    "                        if pd.api.types.is_numeric_dtype(col)\n",
    "                        else np.nan\n",
    "                    ),\n",
    "                    \"50%\": (\n",
    "                        col.quantile(0.5)\n",
    "                        if pd.api.types.is_numeric_dtype(col)\n",
    "                        else np.nan\n",
    "                    ),\n",
    "                    \"75%\": (\n",
    "                        col.quantile(0.75)\n",
    "                        if pd.api.types.is_numeric_dtype(col)\n",
    "                        else np.nan\n",
    "                    ),\n",
    "                    \"max\": col.max() if pd.api.types.is_numeric_dtype(col) else np.nan,\n",
    "                    \"missing\": col.isnull().sum(),\n",
    "                    \"missing_pct\": col.isnull().mean() * 100,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            stats.append(\n",
    "                {\n",
    "                    \"feature\": feat,\n",
    "                    \"count\": 0,\n",
    "                    \"mean\": np.nan,\n",
    "                    \"std\": np.nan,\n",
    "                    \"min\": np.nan,\n",
    "                    \"25%\": np.nan,\n",
    "                    \"50%\": np.nan,\n",
    "                    \"75%\": np.nan,\n",
    "                    \"max\": np.nan,\n",
    "                    \"missing\": len(df),\n",
    "                    \"missing_pct\": 100.0,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(stats).set_index(\"feature\")\n",
    "\n",
    "\n",
    "input_stats = calculate_comprehensive_stats(df_raw, INPUT_FEATURES)\n",
    "print(input_stats.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c365e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominal_capacity_Ah: missing 734/6000\n",
      "eol_cycle_measured: missing 2670/6000\n",
      "capacity_trend_ah: missing 734/6000\n",
      "final_RUL: missing 785/6000\n"
     ]
    }
   ],
   "source": [
    "performance_fields = [\"nominal_capacity_Ah\", \"eol_cycle_measured\", \"capacity_trend_ah\", \"final_RUL\"]\n",
    "\n",
    "for field in performance_fields:\n",
    "    if field in df_raw.columns:\n",
    "        missing = df_raw[field].isnull().sum()\n",
    "        print(f\"{field}: missing {missing}/{len(df_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d40907c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_retention_raw(row):\n",
    "    trend = row.get(\"capacity_trend_ah\")\n",
    "    if trend is None or (isinstance(trend, (list, np.ndarray)) and len(trend) == 0):\n",
    "        return np.nan\n",
    "    trend = np.array(trend)\n",
    "    if trend[0] > 0:\n",
    "        return (trend[-1] / trend[0]) * 100\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "df_raw[\"capacity_retention_percent_raw\"] = df_raw.apply(calculate_retention_raw, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aae66f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retention > 100% rows: 81\n",
      "      sample_id  param_id  capacity_retention_percent_raw\n",
      "422           7         2                      135.304328\n",
      "433           7        13                      130.269119\n",
      "437           7        17                      113.460593\n",
      "452           7        32                      120.163797\n",
      "459           7        39                      130.719000\n",
      "461           7        41                      123.482978\n",
      "462           7        42                      149.947334\n",
      "466           7        46                      121.961749\n",
      "471           7        51                      126.605721\n",
      "2821         47         1                      644.915027\n",
      "2822         47         2                      202.137891\n",
      "2825         47         5                      103.457117\n",
      "2843         47        23                      812.293156\n",
      "2845         47        25                      876.455893\n",
      "2848         47        28                      108.799274\n",
      "2857         47        37                      110.726249\n",
      "2860         47        40                      134.265607\n",
      "2862         47        42                      115.779420\n",
      "2864         47        44                      108.782148\n",
      "2866         47        46                      140.758127\n"
     ]
    }
   ],
   "source": [
    "over_100 = df_raw[df_raw[\"capacity_retention_percent_raw\"] > 100]\n",
    "print(f\"Retention > 100% rows: {len(over_100)}\")\n",
    "print(over_100[[\"sample_id\", \"param_id\", \"capacity_retention_percent_raw\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4161f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IDENTIFYING MISSING EOL_CYCLE_MEASURED SAMPLES\n",
      "================================================================================\n",
      "\n",
      "Total missing eol_cycle_measured: 2,670 rows\n",
      "Percentage: 44.50%\n",
      "\n",
      "First 10 missing eol_cycle_measured samples:\n",
      " sample_id  param_id  eol_cycle_measured\n",
      "         0         7                 NaN\n",
      "         0         8                 NaN\n",
      "         0         9                 NaN\n",
      "         0        10                 NaN\n",
      "         0        13                 NaN\n",
      "         0        14                 NaN\n",
      "         0        15                 NaN\n",
      "         0        21                 NaN\n",
      "         0        25                 NaN\n",
      "         0        34                 NaN\n",
      "\n",
      "✅ Saved to: ../data/missing_eol_cycle_samples.csv\n",
      "   Total rows: 2,670\n",
      "\n",
      "================================================================================\n",
      "VERIFYING IN SOURCE PARQUET FILES\n",
      "================================================================================\n",
      "\n",
      "Verifying first 5 missing samples in source files...\n",
      "\n",
      "Sample 0, Param 7:\n",
      "  Source file: results_rank_0.parquet\n",
      "  eol_cycle_measured in source: nan\n",
      "  Is it NaN/None in source? True\n",
      "  nominal_capacity_Ah: 5.0\n",
      "  capacity_trend_ah length: 26\n",
      "  ✅ CONFIRMED: Missing in source file!\n",
      "\n",
      "Sample 0, Param 8:\n",
      "  Source file: results_rank_0.parquet\n",
      "  eol_cycle_measured in source: nan\n",
      "  Is it NaN/None in source? True\n",
      "  nominal_capacity_Ah: 5.0\n",
      "  capacity_trend_ah length: 28\n",
      "  ✅ CONFIRMED: Missing in source file!\n",
      "\n",
      "Sample 0, Param 9:\n",
      "  Source file: results_rank_0.parquet\n",
      "  eol_cycle_measured in source: nan\n",
      "  Is it NaN/None in source? True\n",
      "  nominal_capacity_Ah: 5.0\n",
      "  capacity_trend_ah length: 21\n",
      "  ✅ CONFIRMED: Missing in source file!\n",
      "\n",
      "Sample 0, Param 10:\n",
      "  Source file: results_rank_0.parquet\n",
      "  eol_cycle_measured in source: nan\n",
      "  Is it NaN/None in source? True\n",
      "  nominal_capacity_Ah: 5.0\n",
      "  capacity_trend_ah length: 43\n",
      "  ✅ CONFIRMED: Missing in source file!\n",
      "\n",
      "Sample 0, Param 13:\n",
      "  Source file: results_rank_0.parquet\n",
      "  eol_cycle_measured in source: nan\n",
      "  Is it NaN/None in source? True\n",
      "  nominal_capacity_Ah: 5.0\n",
      "  capacity_trend_ah length: 217\n",
      "  ✅ CONFIRMED: Missing in source file!\n",
      "\n",
      "================================================================================\n",
      "MISSING EOL STATISTICS BY SAMPLE\n",
      "================================================================================\n",
      "\n",
      "Samples with missing eol_cycle_measured: 60 out of 100\n",
      "\n",
      "Top 10 samples with most missing eol:\n",
      " sample_id  missing_count\n",
      "        45             60\n",
      "        77             60\n",
      "        38             60\n",
      "        39             60\n",
      "        40             60\n",
      "        88             60\n",
      "        43             60\n",
      "        86             60\n",
      "        76             60\n",
      "        29             60\n",
      "\n",
      "Missing eol distribution per sample:\n",
      "count    60.000000\n",
      "mean     44.500000\n",
      "std      19.862664\n",
      "min       1.000000\n",
      "25%      25.750000\n",
      "50%      58.500000\n",
      "75%      60.000000\n",
      "max      60.000000\n",
      "Name: missing_count, dtype: float64\n",
      "\n",
      "⚠️  Samples with ALL 60 params missing eol: 28\n",
      "Sample IDs: [45, 77, 38, 39, 40, 88, 43, 86, 76, 29, 75, 54, 74, 57, 68, 59, 37, 66, 5, 10, 12, 28, 26, 24, 23, 90, 15, 91]\n",
      "\n",
      "================================================================================\n",
      "CROSS-CHECK: CAPACITY DATA FOR MISSING EOL SAMPLES\n",
      "================================================================================\n",
      "\n",
      "Missing eol samples with capacity_trend_ah data: 1,936\n",
      "Missing eol samples WITHOUT capacity_trend_ah: 734\n",
      "\n",
      "Sample rows (missing eol, but HAS capacity data):\n",
      "  Sample 0, Param 7: capacity_trend length = 26\n",
      "  Sample 0, Param 8: capacity_trend length = 28\n",
      "  Sample 0, Param 9: capacity_trend length = 21\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IDENTIFY MISSING EOL SAMPLES AND VERIFY IN SOURCE FILES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"IDENTIFYING MISSING EOL_CYCLE_MEASURED SAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Filter rows where eol_cycle_measured is missing\n",
    "missing_eol = df_raw[df_raw[\"eol_cycle_measured\"].isnull()][\n",
    "    [\"sample_id\", \"param_id\", \"eol_cycle_measured\"]\n",
    "].copy()\n",
    "\n",
    "print(f\"Total missing eol_cycle_measured: {len(missing_eol):,} rows\")\n",
    "print(f\"Percentage: {len(missing_eol)/len(df_raw)*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Show first 10 rows\n",
    "print(\"First 10 missing eol_cycle_measured samples:\")\n",
    "print(missing_eol.head(10).to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "output_file = BASE_DIR / \"data\" / \"missing_eol_cycle_samples.csv\"\n",
    "missing_eol.to_csv(output_file, index=False)\n",
    "print(f\"\\n✅ Saved to: {output_file}\")\n",
    "print(f\"   Total rows: {len(missing_eol):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY IN SOURCE PARQUET FILES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFYING IN SOURCE PARQUET FILES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Sample a few missing samples to verify\n",
    "sample_to_verify = missing_eol.head(5)\n",
    "\n",
    "print(\"Verifying first 5 missing samples in source files...\\n\")\n",
    "\n",
    "for idx, row in sample_to_verify.iterrows():\n",
    "    sample_id = int(row[\"sample_id\"])\n",
    "    param_id = int(row[\"param_id\"])\n",
    "\n",
    "    # Load source parquet file\n",
    "    pq_file = parquet_dir / parquet_pattern.format(sample_id)\n",
    "\n",
    "    if pq_file.exists():\n",
    "        df_source = pd.read_parquet(pq_file)\n",
    "\n",
    "        # Find the specific param\n",
    "        param_row = df_source[df_source[\"param_id\"] == param_id]\n",
    "\n",
    "        if len(param_row) > 0:\n",
    "            eol_value = param_row[\"eol_cycle_measured\"].values[0]\n",
    "\n",
    "            print(f\"Sample {sample_id}, Param {param_id}:\")\n",
    "            print(f\"  Source file: {pq_file.name}\")\n",
    "            print(f\"  eol_cycle_measured in source: {eol_value}\")\n",
    "            print(f\"  Is it NaN/None in source? {pd.isna(eol_value)}\")\n",
    "\n",
    "            # Also check other fields for context\n",
    "            if \"nominal_capacity_Ah\" in param_row.columns:\n",
    "                nom_cap = param_row[\"nominal_capacity_Ah\"].values[0]\n",
    "                print(f\"  nominal_capacity_Ah: {nom_cap}\")\n",
    "\n",
    "            if \"capacity_trend_ah\" in param_row.columns:\n",
    "                cap_trend = param_row[\"capacity_trend_ah\"].values[0]\n",
    "                if isinstance(cap_trend, (list, np.ndarray)):\n",
    "                    print(f\"  capacity_trend_ah length: {len(cap_trend)}\")\n",
    "                else:\n",
    "                    print(f\"  capacity_trend_ah: {cap_trend}\")\n",
    "\n",
    "            print(f\"  ✅ CONFIRMED: Missing in source file!\\n\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Sample {sample_id}, Param {param_id}: ❌ Not found in source file\\n\"\n",
    "            )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Sample {sample_id}, Param {param_id}: ❌ Source file does not exist: {pq_file}\\n\"\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTICS BY SAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MISSING EOL STATISTICS BY SAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Count missing eol per sample\n",
    "missing_by_sample = (\n",
    "    missing_eol.groupby(\"sample_id\").size().reset_index(name=\"missing_count\")\n",
    ")\n",
    "missing_by_sample = missing_by_sample.sort_values(\"missing_count\", ascending=False)\n",
    "\n",
    "print(\n",
    "    f'Samples with missing eol_cycle_measured: {len(missing_by_sample)} out of {df_raw[\"sample_id\"].nunique()}'\n",
    ")\n",
    "print(f\"\\nTop 10 samples with most missing eol:\")\n",
    "print(missing_by_sample.head(10).to_string(index=False))\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nMissing eol distribution per sample:\")\n",
    "print(missing_by_sample[\"missing_count\"].describe())\n",
    "\n",
    "# Check if any samples have ALL params missing eol\n",
    "samples_all_missing = missing_by_sample[\n",
    "    missing_by_sample[\"missing_count\"] == PARAMS_PER_SAMPLE\n",
    "]\n",
    "if len(samples_all_missing) > 0:\n",
    "    print(\n",
    "        f\"\\n⚠️  Samples with ALL {PARAMS_PER_SAMPLE} params missing eol: {len(samples_all_missing)}\"\n",
    "    )\n",
    "    print(\"Sample IDs:\", samples_all_missing[\"sample_id\"].tolist())\n",
    "else:\n",
    "    print(f\"\\n✅ No samples have ALL params missing eol\")\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-CHECK: Do samples with missing eol have capacity data?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-CHECK: CAPACITY DATA FOR MISSING EOL SAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check if missing eol samples have capacity_trend_ah\n",
    "missing_eol_full = df_raw[df_raw[\"eol_cycle_measured\"].isnull()].copy()\n",
    "\n",
    "if \"capacity_trend_ah\" in missing_eol_full.columns:\n",
    "    # Check how many have empty capacity_trend_ah\n",
    "    missing_eol_full[\"has_cap_trend\"] = missing_eol_full[\"capacity_trend_ah\"].apply(\n",
    "        lambda x: (\n",
    "            False\n",
    "            if (x is None or (isinstance(x, (list, np.ndarray)) and len(x) == 0))\n",
    "            else True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'Missing eol samples with capacity_trend_ah data: {missing_eol_full[\"has_cap_trend\"].sum():,}'\n",
    "    )\n",
    "    print(\n",
    "        f'Missing eol samples WITHOUT capacity_trend_ah: {(~missing_eol_full[\"has_cap_trend\"]).sum():,}'\n",
    "    )\n",
    "\n",
    "    # Show a few examples\n",
    "    print(\"\\nSample rows (missing eol, but HAS capacity data):\")\n",
    "    has_both_issues = missing_eol_full[missing_eol_full[\"has_cap_trend\"] == True].head(\n",
    "        3\n",
    "    )\n",
    "    if len(has_both_issues) > 0:\n",
    "        for idx, row in has_both_issues.iterrows():\n",
    "            cap_trend = row[\"capacity_trend_ah\"]\n",
    "            print(\n",
    "                f'  Sample {row[\"sample_id\"]}, Param {row[\"param_id\"]}: capacity_trend length = {len(cap_trend)}'\n",
    "            )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5003d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indiaai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
